# Generator - Synthetic QA Pair Generator

Generate high-quality question-answer pairs from LanceDB chunks for LLM fine-tuning.

## ğŸ¯ Features

- **Multi-provider LLM support**: Ollama, Claude SDK, Google ADK, vLLM, OpenAI, Anthropic
- **Modular client architecture**: Each LLM provider in separate, maintainable modules
- **Instruction Backtranslation**: Treat documents as "answers", generate "questions"
- **Multiple export formats**: ChatML, Alpaca, ShareGPT, JSONL
- **Progress tracking**: Rich progress bars and status messages
- **Batch processing**: Efficient chunked processing with intermediate saves

## ğŸ“¦ Installation

```bash
# Using uv (recommended)
uv pip install -e .

# With cloud providers
uv pip install -e ".[cloud]"

# With all providers
uv pip install -e ".[all]"
```

## ğŸš€ Quick Start

### 1. Configure LLM Provider

Edit `configs/config.yaml`:

```yaml
llm:
  provider: ollama
  model: mistral:latest
  base_url: http://localhost:11434
  temperature: 0.7
```

### 2. Generate QA Pairs

```bash
# Generate from LanceDB
uv run python -m generator.cli generate \
  /path/to/lancedb \
  -o output/qa_raw.json \
  --n-pairs 5 \
  --batch-size 50
```

## ğŸ¨ LLM Provider Setup

### Ollama (Local)

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull model
ollama pull mistral:latest

# Verify
ollama list
```

### Claude SDK (CLI-based)

```bash
# Install SDK
uv pip install ".[cloud]"

# Login to Claude
claude auth login

# Verify
claude auth status
```

### Google ADK (API)

```bash
# Install ADK
uv pip install ".[cloud]"

# Get API key at: https://aistudio.google.com/apikey
export GOOGLE_API_KEY="your-key-here"
```

### OpenAI/Anthropic APIs

```bash
# Install packages
uv pip install ".[cloud]"

# Set API keys
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
```

## ğŸ—ï¸ Architecture

```
src/generator/
â”œâ”€â”€ clients/          # Modular LLM clients
â”‚   â”œâ”€â”€ base.py      # Abstract base class
â”‚   â”œâ”€â”€ ollama.py    # Ollama implementation
â”‚   â”œâ”€â”€ claude.py    # Claude SDK
â”‚   â”œâ”€â”€ google_adk.py # Google Gemini
â”‚   â”œâ”€â”€ vllm.py      # vLLM
â”‚   â”œâ”€â”€ openai.py    # OpenAI
â”‚   â””â”€â”€ anthropic.py # Anthropic
â”œâ”€â”€ qa_generator.py  # QA generation logic
â””â”€â”€ __init__.py      # Package exports
```

### Using Clients in Code

```python
from generator.clients import get_client

# Create client using factory
client = get_client("ollama", {
    "model": "mistral:latest",
    "base_url": "http://localhost:11434",
    "temperature": 0.7,
    "max_tokens": 4096
})

# Generate text
response = client.generate("What is machine learning?")
```

## ğŸ”¬ Methodology

Based on **Instruction Backtranslation** (Meta AI, ICLR 2024):
- Treat document chunks as "answers"
- LLM generates relevant "questions"
- Creates natural instruction-response pairs

## ğŸ“„ License

MIT
