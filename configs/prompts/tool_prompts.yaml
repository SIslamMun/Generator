# ============================================================================
# TOOL USE PROMPTS - Phase 3
# Based on Toolformer, Gorilla, and ToolLLM methodologies
# ============================================================================

# ------------------------------------------------------------------------------
# INSTRUCTION GENERATION
# Generate diverse user instructions for tool usage
# ------------------------------------------------------------------------------
tool_instruction_generation: |
  Generate diverse user instructions that would require this tool.

  Tool: {tool_name}
  Description: {tool_description}
  Parameters: {parameters}
  Examples: {examples}

  Generate {n_instructions} diverse instructions including:
  - Simple direct usage (e.g., "Calculate 15% of 200")
  - Complex parameter combinations (e.g., "Find the square root of the sum of 144 and 256")
  - Edge cases (e.g., "What happens if I divide by zero?")
  - Real-world scenarios (e.g., "I need to split a $127.50 bill among 3 people")

  Return JSON array:
  [
    {{
      "instruction": "Natural language user request",
      "difficulty": "simple|medium|complex",
      "scenario": "Brief context explaining the use case"
    }}
  ]

  Return ONLY valid JSON, no explanation.

# ------------------------------------------------------------------------------
# MULTI-TOOL INSTRUCTION GENERATION
# Generate instructions requiring multiple tools
# ------------------------------------------------------------------------------
multi_tool_instruction_generation: |
  Generate a realistic user instruction that requires using MULTIPLE tools together.

  Available Tools:
  {tools_documentation}

  The instruction should:
  1. Require 2-3 tools to complete
  2. Have a logical flow where outputs chain together
  3. Be a realistic task a user might actually need

  Return JSON:
  {{
    "instruction": "Natural language user request",
    "difficulty": "medium|complex",
    "scenario": "Brief context",
    "expected_tools": ["tool1", "tool2"]
  }}

  Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# SOLUTION ANNOTATION - SINGLE STEP (Unified)
# Single tool call with documentation grounding
# ------------------------------------------------------------------------------
tool_solution_single: |
  Use this API documentation for reference:

  {api_documentation}

  ---

  User Request: {instruction}

  Generate the correct API call to fulfill this request.
  Ground your answer in the documentation above.

  Think step by step:
  1. What is the user asking for?
  2. Which tool can fulfill this request?
  3. What arguments should be passed?

  Respond with JSON:
  {{
    "thought": "Reasoning about which API to use and why",
    "tool": "api_name",
    "args": {{"param1": "value1", "param2": "value2"}},
    "expected_result": "Expected output based on API description",
    "final_answer": "Natural language answer to give the user"
  }}

  Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# SOLUTION ANNOTATION - MULTI STEP (Unified)
# Multi-step reasoning with documentation and chaining
# ------------------------------------------------------------------------------
tool_solution_multi: |
  You are an expert at solving tasks using available tools.

  API Documentation:
  {api_documentation}

  User Instruction: {instruction}

  Available Tools (JSON Schema):
  {tools_json}

  Generate a step-by-step solution using the available tools.
  For each step:
  1. Think about what needs to be done next
  2. Choose the appropriate tool from the documentation
  3. Specify the arguments
  4. Predict the expected result (which may feed into next step)

  Return JSON:
  {{
    "reasoning_path": [
      {{
        "step": 1,
        "thought": "Why this step is needed",
        "tool": "tool_name",
        "args": {{"param": "value"}},
        "expected_result": "What we expect back"
      }},
      {{
        "step": 2,
        "thought": "Next step based on previous result",
        "tool": "another_tool",
        "args": {{}},
        "expected_result": "..."
      }}
    ],
    "final_answer": "Natural language answer that would be given to the user"
  }}

  Maximum {max_steps} steps. Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# LEGACY: SOLUTION ANNOTATION - TOOLFORMER STYLE
# Single-step tool call with reasoning
# ------------------------------------------------------------------------------
tool_solution_toolformer: |
  Given this user request, determine which tool to use and generate the tool call.

  User Request: {instruction}

  Available Tools:
  {tools_list}

  Think step by step:
  1. What is the user asking for?
  2. Which tool can fulfill this request?
  3. What arguments should be passed?

  Respond with JSON:
  {{
    "thought": "Reasoning about why this tool is needed",
    "tool": "tool_name",
    "args": {{"param1": "value1", "param2": "value2"}},
    "expected_result": "Description of expected output"
  }}

  Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# LEGACY: SOLUTION ANNOTATION - GORILLA STYLE
# With API documentation context
# ------------------------------------------------------------------------------
tool_solution_gorilla: |
  Use this API documentation for reference:

  {api_documentation}

  ---

  User Request: {instruction}

  Generate the correct API call to fulfill this request. Ground your answer in the documentation.

  Respond with JSON:
  {{
    "thought": "Reasoning about which API to use and why, referencing the documentation",
    "tool": "api_name",
    "args": {{"param1": "value1"}},
    "expected_result": "Expected output based on API description"
  }}

  Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# LEGACY: SOLUTION ANNOTATION - TOOLLLM STYLE (DFSDT)
# Multi-step reasoning with backtracking capability
# ------------------------------------------------------------------------------
tool_solution_toolllm: |
  You are an expert at solving tasks using available tools.

  User Instruction: {instruction}

  Available Tools:
  {tools_json}

  Generate a step-by-step solution using the available tools.
  For each step:
  1. Think about what needs to be done next
  2. Choose the appropriate tool
  3. Specify the arguments
  4. Predict the expected result

  If a step might fail, consider alternatives (DFSDT approach).

  Return JSON:
  {{
    "reasoning_path": [
      {{
        "step": 1,
        "thought": "Why this step is needed",
        "tool": "tool_name",
        "args": {{"param": "value"}},
        "expected_result": "What we expect back"
      }},
      {{
        "step": 2,
        "thought": "Next step based on previous result",
        "tool": "another_tool",
        "args": {{}},
        "expected_result": "..."
      }}
    ],
    "final_answer": "Natural language answer that would be given to the user"
  }}

  Maximum {max_steps} steps. Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# QUALITY RATING - LLM-as-Judge
# Based on AlpaGasus methodology
# ------------------------------------------------------------------------------
tool_quality_rating: |
  You are an expert evaluator of tool-calling examples for LLM training.

  Rate this example on a scale of 1-10.

  **Instruction:** {instruction}

  **Tool Usage:**
  {tool_calls}

  **Execution Result:** {result}

  **Final Answer:** {answer}

  **Rating Criteria:**
  - Tool Selection (0-3): Are the right tools chosen for the task?
  - Parameter Correctness (0-3): Are arguments valid and appropriate?
  - Reasoning Quality (0-2): Is the thought process clear and logical?
  - Result Usefulness (0-2): Does the result actually solve the instruction?

  **Scoring Guide:**
  - 9-10: Perfect - correct tool, valid params, clear reasoning, useful result
  - 7-8: Good - minor issues but fundamentally correct
  - 5-6: Acceptable - works but has noticeable problems
  - 3-4: Poor - significant issues with tool choice or parameters
  - 1-2: Unusable - completely wrong or broken

  Return JSON:
  {{
    "rating": 8,
    "tool_selection": 3,
    "parameter_correctness": 3,
    "reasoning_quality": 1,
    "result_usefulness": 2,
    "reasoning": "Brief explanation of the rating"
  }}

  Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# STEP QUALITY RATING (Turn-Level Filtering)
# Based on ToolMind (Nov 2025): Rate individual reasoning steps
# Paper: https://arxiv.org/abs/2511.15718
# ------------------------------------------------------------------------------
step_quality_rating: |
  Rate this individual reasoning step for tool-use training data quality.
  
  **User Instruction:** {instruction}
  
  **Step {step_number} of {total_steps}:**
  - Thought: {thought}
  - Tool: {tool}
  - Arguments: {args}
  - Result: {result}
  
  **Previous Context:**
  {previous_context}
  
  **Evaluation Criteria:**
  1. Is the thought logically sound given the instruction and previous context?
  2. Is the tool choice appropriate for this step?
  3. Are the arguments correct, complete, and properly typed?
  4. Does the result make sense for the given arguments?
  5. Does this step advance toward the goal without introducing errors?
  
  **Rating Guide:**
  - 0.9-1.0: Excellent - perfect reasoning, correct tool and args
  - 0.7-0.8: Good - minor issues but fundamentally sound
  - 0.5-0.6: Acceptable - works but has noticeable problems
  - 0.3-0.4: Poor - significant issues that compound errors
  - 0.0-0.2: Bad - wrong tool, invalid args, or misleading
  
  Return JSON:
  {{
    "rating": 0.85,
    "issues": ["list", "of", "problems", "if any"],
    "is_misleading": false,
    "reasoning": "Brief explanation of the rating"
  }}
  
  Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# SEMANTIC VERIFICATION
# Verify execution result semantically solves the instruction
# ------------------------------------------------------------------------------
tool_semantic_verification: |
  Evaluate if this tool execution correctly solves the user's request.

  **User Request:** {instruction}

  **Execution Steps:**
  {steps_summary}

  **Final Answer:** {final_answer}

  Consider:
  1. Were the right tools selected for this task?
  2. Are the execution results relevant to the request?
  3. Does the final answer accurately address the user's need?
  4. Is there any hallucination or made-up information?

  Respond with JSON:
  {{
    "correct": true,
    "confidence": 0.95,
    "issues": [],
    "reasoning": "Brief explanation"
  }}

  If NOT correct:
  {{
    "correct": false,
    "confidence": 0.85,
    "issues": ["Issue 1", "Issue 2"],
    "reasoning": "Why this doesn't correctly solve the request"
  }}

  Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# TOOL SIMULATION - Multi-Agent
# For simulating tool responses when real execution isn't available
# ------------------------------------------------------------------------------
tool_simulation: |
  You are simulating the {tool_name} API.

  Your role is to act AS the API and return realistic responses.

  API Specification:
  - Name: {tool_name}
  - Description: {tool_description}
  - Expected return type: {return_type}

  API Call: {tool_name}({args_json})

  Context from previous calls: {context}

  Generate a realistic response that this API would return.
  The response should:
  1. Be consistent with the API description
  2. Use the provided arguments appropriately
  3. Return the correct data type
  4. Be plausible (not obviously fake data)

  Return ONLY the API response (JSON format if object/array, plain value if primitive).

# ------------------------------------------------------------------------------
# CHAIN-FIRST GENERATION (ToolGrad Aug 2025)
# Generate valid tool chains first, then synthesize queries
# Reduces invalid samples by 40%+ compared to query-first
# ------------------------------------------------------------------------------

chain_generation: |
  Generate a VALID tool chain where each step builds on previous results.

  Available Tools:
  {tools_json}

  Requirements:
  - Generate {min_steps} to {max_steps} steps
  - Each step must use a real tool from the list
  - Arguments must be valid for that tool
  - Steps should chain together logically (output from one feeds into next)
  - Use $step_N_result notation to reference previous results

  Example chain structure:
  - Step 1: Get some data → result is available as $step_1_result
  - Step 2: Process $step_1_result → result is $step_2_result
  - Step 3: Use $step_2_result to produce final output

  Return JSON:
  {{
    "steps": [
      {{
        "step": 1,
        "thought": "Why this tool and these arguments",
        "tool": "tool_name",
        "args": {{"param": "value"}},
        "expected_result": "What this step produces"
      }},
      {{
        "step": 2,
        "thought": "Using result from step 1 to do X",
        "tool": "another_tool",
        "args": {{"input": "$step_1_result", "other": "value"}},
        "expected_result": "What this produces"
      }}
    ],
    "final_answer": "What the complete chain accomplishes"
  }}

  IMPORTANT:
  - Use only tools from the list above
  - Arguments must match tool parameter names exactly
  - The chain must be executable in sequence

  Return ONLY valid JSON.

query_synthesis: |
  Create a natural user query that would require this tool chain.

  Tool Chain:
  {chain_steps}

  Tools Used: {tools_used}
  Final Result: {final_result}

  Generate a realistic user request that:
  1. Would naturally lead to this sequence of tool calls
  2. Sounds like something a real user would ask
  3. Does NOT mention the specific tools or steps
  4. Is clear about what the user wants to accomplish

  Return JSON:
  {{
    "query": "Natural language user request"
  }}

  Return ONLY valid JSON.

# ------------------------------------------------------------------------------
# ERROR HANDLING ANNOTATION
# Generate examples with error recovery
# ------------------------------------------------------------------------------
tool_error_handling: |
  Generate a tool-use example that includes error handling and recovery.

  User Instruction: {instruction}

  Available Tools:
  {tools_list}

  Create a scenario where:
  1. The first approach encounters an error
  2. The model recovers and tries an alternative
  3. Eventually succeeds

  Return JSON:
  {{
    "reasoning_path": [
      {{
        "step": 1,
        "thought": "Initial approach",
        "tool": "tool_name",
        "args": {{}},
        "expected_result": "...",
        "actual_result": {{"error": "Error message"}},
        "status": "failure"
      }},
      {{
        "step": 2,
        "thought": "Recovery: trying alternative approach",
        "tool": "alternative_tool",
        "args": {{}},
        "expected_result": "...",
        "actual_result": {{}},
        "status": "success"
      }}
    ],
    "final_answer": "Answer after recovery"
  }}

  Return ONLY valid JSON.
