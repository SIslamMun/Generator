# ============================================================================
# QA RATING PROMPT (LLM-as-Judge)
# Papers: AlpaGasus (2023), LIMA (2023)
# Rates QA pairs on quality 1-10, filter by threshold
# ============================================================================

prompt: |
  You are an expert evaluator. Rate these question-answer pairs on a scale of 1-10.

  Criteria:
  - Clarity (0-3): Question clear? Answer understandable?
  - Accuracy (0-3): Answer correct and supported by evidence?
  - Usefulness (0-2): Valuable for training end-users (not DevOps)?
  - Difficulty (0-2): Appropriate complexity?

  Rate STRICTLY - most pairs should score 5-7. Only exceptional pairs score 8+.

  **AUTOMATIC LOW SCORES (max score: 3) for:**
  - Questions about GitHub workflows, CI/CD pipelines, or build automation
  - Questions about specific test configurations or runner setups
  - Questions about repository file structure or directory organization
  - Questions about development container or Docker configurations
  - Questions answerable with a single word or version number
  - **Compilation/build questions**: "What happens when you compile...", "How to build...", "Installation steps"
  - **Repository operations**: Cloning, branching, commit hashes, pulling code
  - **Non-technical metadata**: License type, programming language choice, version numbers alone

  **PENALTY RULES:**
  - Infrastructure/CI/CD/Build focus: Reduce usefulness to 0 (max score 6)
  - Compilation/installation without technical API context: Reduce usefulness to 0
  - Too specific (version numbers, file names, commit hashes): Reduce difficulty to 0
  - Overly generic questions without specific component names: Reduce clarity to 1

  For each pair, provide:
  1. Individual criteria scores
  2. Total rating (sum of criteria, max 10)
  3. Brief reasoning explaining the rating

  Return valid JSON with detailed ratings:
  [
    {{
      "question": "Original question",
      "answer": "Original answer",
      "rating": 7,
      "clarity": 2,
      "accuracy": 3,
      "usefulness": 1,
      "difficulty": 1,
      "topic_relevant": true,
      "reasoning": "Clear question with accurate, well-supported answer. Moderately useful for training."
    }},
    {{
      "question": "Original question",
      "answer": "Original answer",
      "rating": 5,
      "clarity": 2,
      "accuracy": 2,
      "usefulness": 1,
      "difficulty": 0,
      "topic_relevant": true,
      "reasoning": "Question is clear but answer lacks depth and complexity."
    }}
  ]

  QA pairs to rate:
  {pairs}
