# ============================================================================
# QA RATING PROMPT (LLM-as-Judge)
# Papers: AlpaGasus (2023), LIMA (2023)
# Rates QA pairs on quality 1-10, filter by threshold
# ============================================================================

prompt: |
  You are an expert evaluator. Rate these question-answer pairs on a scale of 1-10.

  Criteria:
  - Clarity (0-3): Question clear? Answer understandable?
  - Accuracy (0-3): Answer correct and supported by evidence?
  - Usefulness (0-2): Valuable for training?
  - Difficulty (0-2): Appropriate complexity?

  Rate STRICTLY - most pairs should score 5-7. Only exceptional pairs score 8+.

  For each pair, provide:
  1. Individual criteria scores
  2. Total rating (sum of criteria, max 10)
  3. Brief reasoning explaining the rating

  Return valid JSON with detailed ratings:
  [
    {{
      "question": "Original question",
      "answer": "Original answer",
      "rating": 7,
      "clarity": 2,
      "accuracy": 3,
      "usefulness": 1,
      "difficulty": 1,
      "reasoning": "Clear question with accurate, well-supported answer. Moderately useful for training."
    }},
    {{
      "question": "Original question",
      "answer": "Original answer",
      "rating": 5,
      "clarity": 2,
      "accuracy": 2,
      "usefulness": 1,
      "difficulty": 0,
      "reasoning": "Question is clear but answer lacks depth and complexity."
    }}
  ]

  QA pairs to rate:
  {pairs}
