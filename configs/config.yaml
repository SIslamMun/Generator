# ============================================================================
# PHAGOCYTE GENERATOR CONFIGURATION
# Guide: Choose your LLM provider and configure settings
# Usage: Edit this file OR override with CLI flags (--provider, --model)
# ============================================================================

# ============================================================================
# LLM PROVIDER SETTINGS
# ============================================================================
# Choose ONE provider by uncommenting its section and setting provider: <name>
# ============================================================================

llm:
  # ┌─────────────────────────────────────────────────────────────────────┐
  # │ CURRENT ACTIVE PROVIDER - Change this line to switch providers     │
  # └─────────────────────────────────────────────────────────────────────┘
  provider: gemini            # Options: ollama, claude, gemini, vllm, openai, anthropic
  
  # ┌─────────────────────────────────────────────────────────────────────┐
  # │ 1. OLLAMA - Local LLM (RECOMMENDED for unlimited free usage) ⭐     │
  # │    Setup: ollama pull mistral:latest                                │
  # │    Docs: https://ollama.com                                         │
  # └─────────────────────────────────────────────────────────────────────┘
  ollama:
    base_url: http://localhost:11434
    model: mistral:latest       # Available: mistral, llama3.3, qwen2.5, etc.
  
  # ┌─────────────────────────────────────────────────────────────────────┐
  # │ 2. CLAUDE - Claude API (PAID, requires API key)                     │
  # │    Setup: export ANTHROPIC_API_KEY="sk-ant-..."                     │
  # │    Get Key: https://console.anthropic.com/                          │
  # │    Rate Limit: Varies by tier                                       │
  # └─────────────────────────────────────────────────────────────────────┘
  claude:
    model: claude-sonnet-4-20250514
    api_key: ${ANTHROPIC_API_KEY}
  
  # ┌─────────────────────────────────────────────────────────────────────┐
  # │ 3. GEMINI - Google Gemini API (FREE tier: 15 req/min) ⭐            │
  # │    Setup: pip install google-generativeai                           │
  # │    Get Key: https://aistudio.google.com/apikey (FREE)               │
  # │    Set Key: export GOOGLE_API_KEY="your-key"                        │
  # │    Rate Limits (Free):                                               │
  # │      - gemini-1.5-flash: 15 RPM, 1500 RPD (BEST for bulk)           │
  # │      - gemini-2.0-flash-exp: 10 RPM, 1500 RPD                       │
  # │      - gemini-1.5-pro: 2 RPM, 50 RPD (avoid for bulk)               │
  # └─────────────────────────────────────────────────────────────────────┘
  gemini:
    model: gemini-1.5-flash  # Changed from gemini-2.0-flash-exp for better rate limits (15 RPM vs 10 RPM)
    api_key: ${GOOGLE_API_KEY}
  
  # ┌─────────────────────────────────────────────────────────────────────┐
  # │ 4. VLLM - Local vLLM Server (FREE, requires GPU setup)              │
  # │    Setup: vllm serve Qwen/Qwen2.5-72B-Instruct                      │
  # │    Docs: https://docs.vllm.ai                                       │
  # └─────────────────────────────────────────────────────────────────────┘
  vllm:
    base_url: http://localhost:8000/v1
    model: Qwen/Qwen2.5-72B-Instruct
  
  # ┌─────────────────────────────────────────────────────────────────────┐
  # │ 5. OPENAI - OpenAI API (PAID, requires API key)                     │
  # │    Setup: export OPENAI_API_KEY="sk-..."                            │
  # │    Pricing: https://openai.com/pricing                              │
  # └─────────────────────────────────────────────────────────────────────┘
  openai:
    model: gpt-4o-mini              # Options: gpt-4o, gpt-4o-mini, gpt-4-turbo
    api_key: ${OPENAI_API_KEY}
  
  # ┌─────────────────────────────────────────────────────────────────────┐
  # │ 6. ANTHROPIC - Anthropic API (PAID, requires API key)               │
  # │    Setup: export ANTHROPIC_API_KEY="sk-ant-..."                     │
  # │    Pricing: https://anthropic.com/pricing                           │
  # │    Note: Can use Claude SDK for free (see option 2)                 │
  # └─────────────────────────────────────────────────────────────────────┘
  anthropic:
    model: claude-sonnet-4-20250514
    api_key: ${ANTHROPIC_API_KEY}
    use_agent_sdk: false
  
  # ┌─────────────────────────────────────────────────────────────────────┐
  # │ GENERATION PARAMETERS (applies to all providers)                    │
  # └─────────────────────────────────────────────────────────────────────┘
  temperature: 0.7
  max_tokens: 4096

# ============================================================================
# GENERATION SETTINGS
# ============================================================================
generation:
  n_pairs_per_chunk: 5        # Number of QA pairs to generate per chunk
                              # Recommended: 5 (good balance of quality/quantity)
                              # Higher values: More pairs but potentially lower quality
  
  batch_size: 50              # Number of chunks to process in one batch
                              # Larger batches = faster but more memory usage
  
  max_retries: 3              # Number of retry attempts on LLM errors
  retry_delay: 1.0            # Seconds to wait between retries

# ============================================================================
# FILTERING SETTINGS (Pre-filtering before curation)
# ============================================================================
filtering:
  # Source files to skip during generation (metadata, logs, UI pages)
  skip_source_patterns:
    - "_log.md"              # Paper retrieval logs
    - "login.md"             # Login/authentication pages
    - "retrieval_progress"   # Retrieval progress tracking
    - "signup"               # Signup pages
  
  # Question patterns to filter out during curation
  skip_question_patterns:
    # Author/affiliation metadata
    - "email.*address"
    - '@.*\.edu'
    - '@.*\.com'
    - '@.*\.org'
    - "affiliated with"
    - "affiliation"
    - "department.*work"
    - "university.*located"
    - "laboratory.*located"
    
    # UI/website-specific
    - "cookie"
    - "password.*reset"
    - "login.*url"
    - "signup"
    - "atlassian.*cookie"
    - "jira.*service"
    - "helpdesk.*support.*hours"
    - "business.*week"
    
    # Document metadata (not technical content)
    - "DOI.*completed.*item"
    - "retrieval.*status"
    - "paper.*being.*retrieved"
    - "successfully.*downloaded"
  
  # Minimum content length for chunks (characters)
  min_chunk_length: 300          # Increased from 200 to filter short/problematic chunks
  
  # Enable/disable filtering
  enabled: true

# ============================================================================
# CURATION SETTINGS (LLM-as-Judge Quality Filtering)
# Based on: AlpaGasus (2023) + LIMA (2023) papers
# ============================================================================
curate:
  provider: claude            # LLM judge provider (claude recommended for quality assessment)
                              # Options: ollama, claude, gemini, vllm, openai, anthropic
  
  threshold: 7.0              # Minimum rating to keep (1-10 scale)
                              # Recommended ranges:
                              #   - 6.0-6.5: More pairs, moderate quality (60-80% retention)
                              #   - 7.0: Balanced quality (40-60% retention) ⭐ RECOMMENDED
                              #   - 8.0+: Strict quality (20-40% retention, LIMA-style)
  
  batch_size: 5               # Number of QA pairs to rate per LLM call
                              # Smaller batches = more API calls but better accuracy
  
  temperature: 0.1            # Low temperature for consistent rating
                              # Keep low (0.1-0.2) for judge reliability

# ============================================================================
# ENRICHMENT SETTINGS (Response Improvement)
# ============================================================================
enrich:
  batch_size: 5               # Number of QA pairs to enrich per batch
  temperature: 0.3            # Moderate temperature for creative improvements
                              # Range: 0.2-0.4 for controlled variation
  preserve_original: true     # Keep original answer if enrichment fails

# ============================================================================
# CHAIN-OF-THOUGHT (CoT) SETTINGS
# ============================================================================
cot:
  temperature: 0.2            # Low temperature for logical reasoning
                              # Keep low (0.1-0.3) for step-by-step clarity
  batch_size: 5               # Number of pairs to enhance with reasoning

# ============================================================================
# RATE LIMITING (Prevent API Quota Issues)
# ============================================================================
rate_limit:
  delay_between_requests: 4   # Seconds between API calls (15 RPM = 4s delay for safety)
  save_every_n_pairs: 100     # Save intermediate results every N pairs
  max_retries: 5              # Retry attempts for rate limit errors
  backoff_base: 65            # Base seconds for exponential backoff (65, 130, 260...)

# ============================================================================
# OUTPUT SETTINGS
# ============================================================================
output:
  save_intermediate: true     # Save progress during generation (resume capability)
  lancedb_storage: true       # Store results in LanceDB (version control)
  include_metadata: true      # Include source file, chunk ID, timestamps

# ============================================================================
# PROMPTS CONFIGURATION
# ============================================================================
prompts_file: prompts.yaml    # External prompt templates (easy customization)
